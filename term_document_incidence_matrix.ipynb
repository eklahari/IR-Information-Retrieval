{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMjQygWXG1zKP7Xifiip8Fz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eklahari/IR-Information-Retrieval/blob/main/term_document_incidence_matrix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Term-Document Incidence Matrix**"
      ],
      "metadata": {
        "id": "U-6Rl58T26-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import necessary libraries do **tokenising** and **liguistic preprocessing**"
      ],
      "metadata": {
        "id": "HqRFULKSIUg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_documents(documents):\n",
        "    refined_tokens = []\n",
        "\n",
        "    for doc in documents:\n",
        "        # Step 1: Lowercasing\n",
        "        doc = doc.lower()\n",
        "\n",
        "        # Step 2: Tokenization\n",
        "        tokens = word_tokenize(doc)\n",
        "\n",
        "        # Step 3: Removing Punctuation\n",
        "        tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "        # Step 4: Stopword Removal\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        # Step 5: Stemming (using Porter Stemmer)\n",
        "        stemmer = PorterStemmer()\n",
        "        stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "        # Step 6: Lemmatization (using WordNet Lemmatizer)\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "        # Extend the refined_tokens list with the current document's tokens\n",
        "        refined_tokens.extend(lemmatized_tokens)\n",
        "\n",
        "    return refined_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMRr0XIGCoav",
        "outputId": "71b135da-fb49-4b9d-e8f1-6c99f6a5a486"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**construct** term-document incidence matrix"
      ],
      "metadata": {
        "id": "UwsIT4HZJfQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_term_document_matrix(documents):\n",
        "    # Preprocess documents\n",
        "    refined_tokens = preprocess_documents(documents)\n",
        "\n",
        "    # Create a list of unique terms (words)\n",
        "    unique_terms = list(set(refined_tokens))\n",
        "\n",
        "    # Create an empty term-document incidence matrix\n",
        "    term_document_matrix = []\n",
        "\n",
        "    # Initialize a row for each term and a column for each document\n",
        "    for term in unique_terms:\n",
        "        row = []\n",
        "        for doc in documents:\n",
        "            # Count the frequency of the term in the document\n",
        "            term_count = doc.split().count(term)\n",
        "            row.append(term_count)\n",
        "        term_document_matrix.append(row)\n",
        "\n",
        "    return unique_terms,term_document_matrix\n",
        "# Example usage:\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog's tail.\",\n",
        "    \"I am learning about text preprocessing.\",\n",
        "    \"Natural language processing is fascinating!\"\n",
        "]\n",
        "unique_terms,term_document_matrix=create_term_document_matrix(documents)\n",
        "# Print the term-document incidence matrix neatly\n",
        "print(\"Term-Document Incidence Matrix:\\n\")\n",
        "# Print header row with document names\n",
        "print(\"\\t\", end=\"\")\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Doc {i+1}\\t\", end=\"\")\n",
        "print()\n",
        "\n",
        "# Print term rows with counts\n",
        "for i, term_row in enumerate(term_document_matrix):\n",
        "    print(unique_terms[i], end=\"\\t\")\n",
        "    for count in term_row:\n",
        "        print(count, end=\"\\t\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cm71hoHJdbr",
        "outputId": "f2377c3f-30d8-430f-953a-b9d7a611bc82"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term-Document Incidence Matrix:\n",
            "\n",
            "\tDoc 1\tDoc 2\tDoc 3\t\n",
            "'s\t0\t0\t0\t\n",
            "text\t0\t1\t0\t\n",
            "preprocessing\t0\t0\t0\t\n",
            "natural\t0\t0\t0\t\n",
            "dog\t0\t0\t0\t\n",
            "learning\t0\t1\t0\t\n",
            "language\t0\t0\t1\t\n",
            "quick\t1\t0\t0\t\n",
            "lazy\t1\t0\t0\t\n",
            "processing\t0\t0\t1\t\n",
            "tail\t0\t0\t0\t\n",
            "fox\t1\t0\t0\t\n",
            "fascinating\t0\t0\t0\t\n",
            "brown\t1\t0\t0\t\n",
            "jump\t0\t0\t0\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "process **boolean queries**"
      ],
      "metadata": {
        "id": "6eC1C7ctJ5Kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_not(operand):\n",
        "    return [0 if x == 1 else 1 for x in operand]\n",
        "\n",
        "def apply_operator(operator, operand1, operand2):\n",
        "    if operator == 'and':\n",
        "        return [a & b for a, b in zip(operand1, operand2)]\n",
        "    elif operator == 'or':\n",
        "        return [a | b for a, b in zip(operand1, operand2)]\n",
        "\n",
        "def process_boolean_query(query, term_document_matrix, unique_terms):\n",
        "    query = query.lower()\n",
        "    query = query.replace(\"(\", \" ( \").replace(\")\", \" ) \").split()\n",
        "\n",
        "    # Define operator precedence\n",
        "    precedence = {'not': 3, 'and': 2, 'or': 1}\n",
        "\n",
        "    # Initialize stacks for operators and operands\n",
        "    operators = []\n",
        "    operands = []\n",
        "\n",
        "    for token in query:\n",
        "        if token in unique_terms:\n",
        "            term_index = unique_terms.index(token)\n",
        "            operands.append(term_document_matrix[term_index])\n",
        "        elif token == '(':\n",
        "            operators.append(token)\n",
        "        elif token == ')':\n",
        "            while operators and operators[-1] != '(':\n",
        "                operator = operators.pop()\n",
        "                if operator == 'not':\n",
        "                    operand = operands.pop()\n",
        "                    result = custom_not(operand)\n",
        "                else:\n",
        "                    operand2 = operands.pop()\n",
        "                    operand1 = operands.pop()\n",
        "                    result = apply_operator(operator, operand1, operand2)\n",
        "\n",
        "                operands.append(result)\n",
        "\n",
        "            operators.pop()  # Remove the '('\n",
        "        elif token in precedence:\n",
        "            while (operators and operators[-1] in precedence and\n",
        "                    precedence[operators[-1]] >= precedence[token]):\n",
        "                operator = operators.pop()\n",
        "                if operator == 'not':\n",
        "                    operand = operands.pop()\n",
        "                    result = custom_not(operand)\n",
        "                else:\n",
        "                    operand2 = operands.pop()\n",
        "                    operand1 = operands.pop()\n",
        "                    result = apply_operator(operator, operand1, operand2)\n",
        "\n",
        "                operands.append(result)\n",
        "\n",
        "            operators.append(token)\n",
        "\n",
        "    # Process remaining operators and operands\n",
        "    while operators:\n",
        "        operator = operators.pop()\n",
        "        if operator == '(' or operator == ')':\n",
        "            raise ValueError(\"Invalid query\")\n",
        "        if operator == 'not':\n",
        "            operand = operands.pop()\n",
        "            result = custom_not(operand)\n",
        "        else:\n",
        "            operand2 = operands.pop()\n",
        "            operand1 = operands.pop()\n",
        "            result = apply_operator(operator, operand1, operand2)\n",
        "\n",
        "        operands.append(result)\n",
        "\n",
        "    return operands[0]\n",
        "\n",
        "\n",
        "\n",
        "# Complex boolean query\n",
        "#sample query :(quick and brown) or not (fox and not tail)\n",
        "query=input(\"enter the query:\")\n",
        "result = process_boolean_query(query, term_document_matrix, unique_terms)\n",
        "\n",
        "# Print the result of the query\n",
        "print(\"\\nQuery:\", query)\n",
        "print(\"Result:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzSuSeAL_JDf",
        "outputId": "67033923-acff-482d-8cef-65c5b86716a7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the query:(quick and lazy) or not(fox)\n",
            "\n",
            "Query: (quick and lazy) or not(fox)\n",
            "Result: [1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FfghqkYsL66o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}