{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+H2hDTzlfCidYsJbP6hAy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eklahari/IR-Information-Retrieval/blob/main/InvertedIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inverted - Index**"
      ],
      "metadata": {
        "id": "kos39R--8ywR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "def preprocess_documents(documents):\n",
        "    refined_tokens = []\n",
        "\n",
        "    for doc in documents:\n",
        "        # Step 1: Lowercasing\n",
        "        doc = doc.lower()\n",
        "\n",
        "        # Step 2: Tokenization\n",
        "        tokens = word_tokenize(doc)\n",
        "\n",
        "        # Step 3: Removing Punctuation\n",
        "        tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "        # Step 4: Stopword Removal\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        # Step 5: Stemming (using Porter Stemmer)\n",
        "        stemmer = PorterStemmer()\n",
        "        stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "        # Step 6: Lemmatization (using WordNet Lemmatizer)\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "        # Extend the refined_tokens list with the current document's tokens\n",
        "        refined_tokens.extend(lemmatized_tokens)\n",
        "\n",
        "    return refined_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj5TFDV1AZhx",
        "outputId": "373ffa9c-e26c-4303-9d4a-c28a7faada3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inverted_index(documents):\n",
        "    inverted_index = {}\n",
        "\n",
        "    # Iterate through documents\n",
        "    for doc_id, doc in enumerate(documents):\n",
        "        # Preprocess the document to get refined tokens\n",
        "        tokens = preprocess_documents([doc])\n",
        "\n",
        "        for term in tokens:\n",
        "            if term not in inverted_index:\n",
        "                inverted_index[term] = []\n",
        "            if doc_id not in inverted_index[term]:\n",
        "                inverted_index[term].append(doc_id)\n",
        "\n",
        "    return inverted_index\n",
        "def print_inverted_index(inverted_index):\n",
        "    print(\"terms||postingslist\")\n",
        "    for term, doc_ids in inverted_index.items():\n",
        "        print(f\"{term}\",end=\"\")\n",
        "        print(\"->\",doc_ids)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "FMXb3kcCBJMj"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_complex_query(query, inverted_index):\n",
        "    def and_operator(postings1, postings2):\n",
        "        p1, p2 = 0, 0\n",
        "        result = []\n",
        "        while p1 < len(postings1) and p2 < len(postings2):\n",
        "            if postings1[p1] == postings2[p2]:\n",
        "                result.append(postings1[p1])\n",
        "                p1 += 1\n",
        "                p2 += 1\n",
        "            elif postings1[p1] < postings2[p2]:\n",
        "                p1 += 1\n",
        "            else:\n",
        "                p2 += 1\n",
        "        return result\n",
        "\n",
        "    def or_operator(postings1, postings2):\n",
        "        p1, p2 = 0, 0\n",
        "        result = []\n",
        "        while p1 < len(postings1) and p2 < len(postings2):\n",
        "            if postings1[p1] == postings2[p2]:\n",
        "                result.append(postings1[p1])\n",
        "                p1 += 1\n",
        "                p2 += 1\n",
        "            elif postings1[p1] < postings2[p2]:\n",
        "                result.append(postings1[p1])\n",
        "                p1 += 1\n",
        "            else:\n",
        "                result.append(postings2[p2])\n",
        "                p2 += 1\n",
        "\n",
        "        while p1 < len(postings1):\n",
        "            result.append(postings1[p1])\n",
        "            p1 += 1\n",
        "\n",
        "        while p2 < len(postings2):\n",
        "            result.append(postings2[p2])\n",
        "            p2 += 1\n",
        "\n",
        "        return result\n",
        "\n",
        "    def not_operator(postings):\n",
        "        all_docs = set(range(1, 5))  # Replace with the actual range of document IDs\n",
        "        return list(all_docs - set(postings))\n",
        "\n",
        "    def solve(postings, operator):\n",
        "        if operator == 'and':\n",
        "            return and_operator(postings[0], postings[1])\n",
        "        elif operator == 'or':\n",
        "            return or_operator(postings[0], postings[1])\n",
        "        else:\n",
        "            return not_operator(postings[0])\n",
        "\n",
        "    query = query.lower()\n",
        "    query = query.replace(\"(\", \" ( \").replace(\")\", \" ) \").split()\n",
        "\n",
        "    operators = []\n",
        "    operands = []\n",
        "\n",
        "    for token in query:\n",
        "        if token in inverted_index:\n",
        "            operand = inverted_index[token]\n",
        "            operands.append(operand)\n",
        "        elif token == '(':\n",
        "            operators.append(token)\n",
        "        elif token == ')':\n",
        "            while operators and operators[-1] != '(':\n",
        "                operator = operators.pop()\n",
        "                operand2 = operands.pop()\n",
        "                operand1 = operands.pop()\n",
        "                result = solve([operand1, operand2], operator)\n",
        "                operands.append(result)\n",
        "            operators.pop()  # Remove the '('\n",
        "        elif token in {'and', 'or', 'not'}:\n",
        "            operators.append(token)\n",
        "\n",
        "    # Process remaining operators and operands\n",
        "    while operators:\n",
        "        operator = operators.pop()\n",
        "        if operator == '(' or operator == ')':\n",
        "            raise ValueError(\"Invalid query\")\n",
        "        operand2 = operands.pop()\n",
        "        operand1 = operands.pop()\n",
        "        result = solve([operand1, operand2], operator)\n",
        "        operands.append(result)\n",
        "\n",
        "    return operands[0]\n"
      ],
      "metadata": {
        "id": "0qLvwI07D94i"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"breakthrough drug for schizophrenia\",\n",
        "    \"new schizophrenia drug\",\n",
        "    \"new approach for treatment of schizophrenia\",\n",
        "    \"new hopes for schizophrenia patients\"\n",
        "    ]\n",
        "\n",
        "# Build the inverted index\n",
        "inverted_index = build_inverted_index(documents)\n",
        "print_inverted_index(inverted_index)\n",
        "\n",
        "# Process different queries\n",
        "queries = [\n",
        "    \"(new and treatment) or (new and approach)\",\n",
        "    \"(breakthrough or approach) and schizophrenia\",\n",
        "    \"schizophrenia\"\n",
        "]\n",
        "for query in queries:\n",
        "    result = process_complex_query(query,inverted_index)\n",
        "    print(\"Query:\", query)\n",
        "    print(\"Result:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdnLMpW1W5-_",
        "outputId": "8b95b9e1-24e7-4f93-cfb4-6251a948292e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "terms||postingslist\n",
            "breakthrough-> [0]\n",
            "drug-> [0, 1]\n",
            "schizophrenia-> [0, 1, 2, 3]\n",
            "new-> [1, 2, 3]\n",
            "approach-> [2]\n",
            "treatment-> [2]\n",
            "hope-> [3]\n",
            "patient-> [3]\n",
            "\n",
            "\n",
            "Query: (new and treatment) or (new and approach)\n",
            "Result: [2]\n",
            "Query: (breakthrough or approach) and schizophrenia\n",
            "Result: [0, 2]\n",
            "Query: schizophrenia\n",
            "Result: [0, 1, 2, 3]\n"
          ]
        }
      ]
    }
  ]
}